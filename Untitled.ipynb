{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "import os\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, GRU, Embedding, Layer\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot\n",
    "\n",
    "from src.helpme import clean_text,start_end_tagger,max_length,tokenize,preprocess,preprocess_sentence,loss_function,train_step,evaluate,plot_attention\n",
    "from src.models import Encoder,Decoder,Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_500 = pd.read_csv('data/lyrics_save500.txt', sep='\\t')\n",
    "df_test = pd.read_csv('data/kor.txt', sep='\\t', names=['eng','kor','drop_me'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_500 = df_500.drop([19565, 28696, 31890])\n",
    "# df_500['lang'] = df_500['kor'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = df_500[df_500['lang']=='ko'].drop(columns=['Unnamed: 0','lang'])\n",
    "\n",
    "df_test = df_test.drop(columns='drop_me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>kor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>가.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>안녕.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>뛰어!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run.</td>\n",
       "      <td>뛰어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who?</td>\n",
       "      <td>누구?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>Tom always cried when his sister took away his...</td>\n",
       "      <td>톰은 누나가 자기 장난감을 빼앗아 갔을 때마다 울음을 터뜨렸고, 누나는 바로 그런 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>Science fiction has undoubtedly been the inspi...</td>\n",
       "      <td>공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>I started a new blog. I'll do my best not to b...</td>\n",
       "      <td>난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>I think it's a shame that some foreign languag...</td>\n",
       "      <td>몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3318 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    eng  \\\n",
       "0                                                   Go.   \n",
       "1                                                   Hi.   \n",
       "2                                                  Run!   \n",
       "3                                                  Run.   \n",
       "4                                                  Who?   \n",
       "...                                                 ...   \n",
       "3313  Tom always cried when his sister took away his...   \n",
       "3314  Science fiction has undoubtedly been the inspi...   \n",
       "3315  I started a new blog. I'll do my best not to b...   \n",
       "3316  I think it's a shame that some foreign languag...   \n",
       "3317  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                    kor  \n",
       "0                                                    가.  \n",
       "1                                                   안녕.  \n",
       "2                                                   뛰어!  \n",
       "3                                                   뛰어.  \n",
       "4                                                   누구?  \n",
       "...                                                 ...  \n",
       "3313  톰은 누나가 자기 장난감을 빼앗아 갔을 때마다 울음을 터뜨렸고, 누나는 바로 그런 ...  \n",
       "3314       공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.  \n",
       "3315  난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...  \n",
       "3316  몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...  \n",
       "3317  의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...  \n",
       "\n",
       "[3318 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     text = re.sub(r\"’\", \"'\", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am\", text)\n",
    "#     text = re.sub(r\"he's\", \"he is\", text)\n",
    "#     text = re.sub(r\"she's\", \"she is\", text)\n",
    "#     text = re.sub(r\"it's\", \"it is\", text)\n",
    "#     text = re.sub(r\"that's\", \"that is\", text)\n",
    "#     text = re.sub(r\"what's\", \"that is\", text)\n",
    "#     text = re.sub(r\"where's\", \"where is\", text)\n",
    "#     text = re.sub(r\"how's\", \"how is\", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are\", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would\", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are\", text)\n",
    "#     text = re.sub(r\"won't\", \"will not\", text)\n",
    "#     text = re.sub(r\"can't\", \"cannot\", text)\n",
    "#     text = re.sub(r\"n't\", \" not\", text)\n",
    "#     text = re.sub(r\"n'\", \"ng\", text)\n",
    "#     text = re.sub(r\"'bout\", \"about\", text)\n",
    "#     text = re.sub(r\"'til\", \"until\", text)\n",
    "#     text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "# #     text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_end_tagger(decoder_input_sentence):\n",
    "#     start_tag = \"<start> \"\n",
    "#     end_tag = \" <end>\"\n",
    "#     final_target = start_tag + decoder_input_sentence + end_tag\n",
    "#     return final_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def max_length(tensor):\n",
    "#     return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(lang):\n",
    "#     lang_tokenizer = Tokenizer(filters='')\n",
    "#     lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "#     tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "#     padded_tensor = pad_sequences(tensor, maxlen=max_length(tensor), padding='post')\n",
    "\n",
    "#     return padded_tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(language):\n",
    "#     language = language.apply(clean_text)\n",
    "    \n",
    "#     language = language.apply(start_end_tagger)\n",
    "    \n",
    "#     return language\n",
    "\n",
    "# def preprocess_sentence(sentence):\n",
    "#     sentence = clean_text(sentence)\n",
    "#     sentence = start_end_tagger(sentence)\n",
    "#     return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = preprocess(df_test['eng'])\n",
    "kor = preprocess(df_test['kor'])\n",
    "\n",
    "input_tensor, input_lang_tokenizer = tokenize(eng)\n",
    "target_tensor, target_lang_tokenizer = tokenize(kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        <start> go <end>\n",
       "1                                        <start> hi <end>\n",
       "2                                       <start> run <end>\n",
       "3                                       <start> run <end>\n",
       "4                                       <start> who <end>\n",
       "                              ...                        \n",
       "3313    <start> tom always cried when his sister took ...\n",
       "3314    <start> science fiction has undoubtedly been t...\n",
       "3315    <start> i started a new blog i will do my best...\n",
       "3316    <start> i think it is a shame that some foreig...\n",
       "3317    <start> doubtless there exists in this world p...\n",
       "Name: eng, Length: 3318, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                         <start> 가 <end>\n",
       "1                                        <start> 안녕 <end>\n",
       "2                                        <start> 뛰어 <end>\n",
       "3                                        <start> 뛰어 <end>\n",
       "4                                        <start> 누구 <end>\n",
       "                              ...                        \n",
       "3313    <start> 톰은 누나가 자기 장난감을 빼앗아 갔을 때마다 울음을 터뜨렸고 누나는...\n",
       "3314    <start> 공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감...\n",
       "3315    <start> 난 블로그를 시작했어 블로그를 초반에만 반짝 많이 하다가 관두는 사람...\n",
       "3316    <start> 몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을...\n",
       "3317    <start> 의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결...\n",
       "Name: kor, Length: 3318, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 1,\n",
       " '<end>': 2,\n",
       " 'i': 3,\n",
       " 'tom': 4,\n",
       " 'is': 5,\n",
       " 'to': 6,\n",
       " 'you': 7,\n",
       " 'a': 8,\n",
       " 'the': 9,\n",
       " 'not': 10,\n",
       " 'do': 11,\n",
       " 'that': 12,\n",
       " 'have': 13,\n",
       " 'in': 14,\n",
       " 'are': 15,\n",
       " 'it': 16,\n",
       " 'am': 17,\n",
       " 'was': 18,\n",
       " 'this': 19,\n",
       " 'my': 20,\n",
       " 'of': 21,\n",
       " 'me': 22,\n",
       " 'be': 23,\n",
       " 'he': 24,\n",
       " 'we': 25,\n",
       " 'like': 26,\n",
       " 'did': 27,\n",
       " 'think': 28,\n",
       " 'there': 29,\n",
       " 'your': 30,\n",
       " 'for': 31,\n",
       " 'want': 32,\n",
       " 'french': 33,\n",
       " 'his': 34,\n",
       " 'how': 35,\n",
       " 'mary': 36,\n",
       " 'will': 37,\n",
       " 'and': 38,\n",
       " 'can': 39,\n",
       " 'what': 40,\n",
       " 'know': 41,\n",
       " 'very': 42,\n",
       " 'would': 43,\n",
       " 'has': 44,\n",
       " 'with': 45,\n",
       " 'at': 46,\n",
       " 'go': 47,\n",
       " 'been': 48,\n",
       " 'going': 49,\n",
       " 'here': 50,\n",
       " 'as': 51,\n",
       " 'really': 52,\n",
       " 'on': 53,\n",
       " 'does': 54,\n",
       " 'why': 55,\n",
       " 'she': 56,\n",
       " 'should': 57,\n",
       " 'they': 58,\n",
       " 'ever': 59,\n",
       " 'all': 60,\n",
       " 'please': 61,\n",
       " 'time': 62,\n",
       " 'help': 63,\n",
       " 'sorry': 64,\n",
       " 'tell': 65,\n",
       " 'cannot': 66,\n",
       " 'so': 67,\n",
       " 'keep': 68,\n",
       " 'something': 69,\n",
       " 'still': 70,\n",
       " 'from': 71,\n",
       " 'if': 72,\n",
       " 'no': 73,\n",
       " 'school': 74,\n",
       " 'had': 75,\n",
       " 'too': 76,\n",
       " \"tom's\": 77,\n",
       " 'where': 78,\n",
       " 'but': 79,\n",
       " 'come': 80,\n",
       " 'hard': 81,\n",
       " 'when': 82,\n",
       " 'get': 83,\n",
       " 'work': 84,\n",
       " 'one': 85,\n",
       " 'lonely': 86,\n",
       " 'used': 87,\n",
       " 'more': 88,\n",
       " 'said': 89,\n",
       " 'could': 90,\n",
       " 'who': 91,\n",
       " 'us': 92,\n",
       " 'good': 93,\n",
       " 'now': 94,\n",
       " 'were': 95,\n",
       " 'up': 96,\n",
       " 'stop': 97,\n",
       " 'take': 98,\n",
       " 'about': 99,\n",
       " 'than': 100,\n",
       " 'favorite': 101,\n",
       " 'some': 102,\n",
       " 'never': 103,\n",
       " 'cat': 104,\n",
       " 'people': 105,\n",
       " 'learn': 106,\n",
       " 'him': 107,\n",
       " 'everyone': 108,\n",
       " 'three': 109,\n",
       " 'speak': 110,\n",
       " 'much': 111,\n",
       " 'thought': 112,\n",
       " 'need': 113,\n",
       " 'water': 114,\n",
       " 'person': 115,\n",
       " 'say': 116,\n",
       " 'just': 117,\n",
       " 'old': 118,\n",
       " 'look': 119,\n",
       " 'alone': 120,\n",
       " 'love': 121,\n",
       " 'yesterday': 122,\n",
       " 'by': 123,\n",
       " 'boston': 124,\n",
       " 'doing': 125,\n",
       " 'home': 126,\n",
       " 'right': 127,\n",
       " 'any': 128,\n",
       " 'science': 129,\n",
       " 'lot': 130,\n",
       " 'an': 131,\n",
       " 'out': 132,\n",
       " 'well': 133,\n",
       " 'many': 134,\n",
       " 'australia': 135,\n",
       " 'lie': 136,\n",
       " 'study': 137,\n",
       " 'dog': 138,\n",
       " 'car': 139,\n",
       " 'human': 140,\n",
       " 'feel': 141,\n",
       " 'everybody': 142,\n",
       " 'only': 143,\n",
       " 'day': 144,\n",
       " 'room': 145,\n",
       " 'quite': 146,\n",
       " 'language': 147,\n",
       " 'live': 148,\n",
       " 'our': 149,\n",
       " 'last': 150,\n",
       " 'world': 151,\n",
       " 'see': 152,\n",
       " 'came': 153,\n",
       " 'died': 154,\n",
       " 'late': 155,\n",
       " 'read': 156,\n",
       " 'money': 157,\n",
       " 'book': 158,\n",
       " 'learned': 159,\n",
       " 'give': 160,\n",
       " 'long': 161,\n",
       " 'told': 162,\n",
       " 'wanted': 163,\n",
       " 'yet': 164,\n",
       " 'most': 165,\n",
       " 'before': 166,\n",
       " 'try': 167,\n",
       " 'got': 168,\n",
       " 'way': 169,\n",
       " 'knows': 170,\n",
       " 'anything': 171,\n",
       " 'blood': 172,\n",
       " 'asked': 173,\n",
       " 'tomorrow': 174,\n",
       " 'use': 175,\n",
       " 'knew': 176,\n",
       " 'cold': 177,\n",
       " 'her': 178,\n",
       " 'made': 179,\n",
       " 'house': 180,\n",
       " 'wants': 181,\n",
       " 'teacher': 182,\n",
       " 'two': 183,\n",
       " 'left': 184,\n",
       " 'again': 185,\n",
       " 'eat': 186,\n",
       " 'god': 187,\n",
       " 'new': 188,\n",
       " 'hate': 189,\n",
       " 'understand': 190,\n",
       " 'suicide': 191,\n",
       " 'buy': 192,\n",
       " 'believe': 193,\n",
       " 'anymore': 194,\n",
       " 'age': 195,\n",
       " 'years': 196,\n",
       " \"let's\": 197,\n",
       " 'leave': 198,\n",
       " 'stay': 199,\n",
       " 'problem': 200,\n",
       " 'nobody': 201,\n",
       " 'beautiful': 202,\n",
       " 'true': 203,\n",
       " 'dark': 204,\n",
       " 'met': 205,\n",
       " 'always': 206,\n",
       " 'same': 207,\n",
       " 'computer': 208,\n",
       " 'learning': 209,\n",
       " 'class': 210,\n",
       " 'other': 211,\n",
       " 'wearing': 212,\n",
       " 'job': 213,\n",
       " 'done': 214,\n",
       " 'tried': 215,\n",
       " 'drive': 216,\n",
       " 'fish': 217,\n",
       " 'busy': 218,\n",
       " 'sleep': 219,\n",
       " 'bed': 220,\n",
       " 'went': 221,\n",
       " 'looks': 222,\n",
       " 'these': 223,\n",
       " 'korea': 224,\n",
       " 'meet': 225,\n",
       " 'into': 226,\n",
       " 'or': 227,\n",
       " 'better': 228,\n",
       " 'having': 229,\n",
       " 'every': 230,\n",
       " 'next': 231,\n",
       " 'software': 232,\n",
       " 'wait': 233,\n",
       " 'back': 234,\n",
       " 'anyone': 235,\n",
       " 'sing': 236,\n",
       " 'talk': 237,\n",
       " 'let': 238,\n",
       " 'trying': 239,\n",
       " 'red': 240,\n",
       " 'cats': 241,\n",
       " 'young': 242,\n",
       " 'happy': 243,\n",
       " 'year': 244,\n",
       " 'dream': 245,\n",
       " 'languages': 246,\n",
       " 'moon': 247,\n",
       " 'working': 248,\n",
       " 'subway': 249,\n",
       " 'man': 250,\n",
       " 'life': 251,\n",
       " 'medicine': 252,\n",
       " 'wrong': 253,\n",
       " 'food': 254,\n",
       " 'after': 255,\n",
       " 'today': 256,\n",
       " 'wish': 257,\n",
       " 'social': 258,\n",
       " 'friends': 259,\n",
       " 'without': 260,\n",
       " 'hours': 261,\n",
       " 'down': 262,\n",
       " 'felt': 263,\n",
       " 'yourself': 264,\n",
       " 'nothing': 265,\n",
       " 'someone': 266,\n",
       " 'saw': 267,\n",
       " 'heard': 268,\n",
       " 'may': 269,\n",
       " 'often': 270,\n",
       " 'night': 271,\n",
       " 'father': 272,\n",
       " 'make': 273,\n",
       " 'sure': 274,\n",
       " 'week': 275,\n",
       " \"mary's\": 276,\n",
       " \"who's\": 277,\n",
       " 'afraid': 278,\n",
       " 'play': 279,\n",
       " 'birthday': 280,\n",
       " 'open': 281,\n",
       " 'off': 282,\n",
       " 'works': 283,\n",
       " 'answer': 284,\n",
       " 'forget': 285,\n",
       " 'start': 286,\n",
       " 'remember': 287,\n",
       " 'talking': 288,\n",
       " 'myself': 289,\n",
       " 'find': 290,\n",
       " 'bought': 291,\n",
       " 'bank': 292,\n",
       " 'dictionary': 293,\n",
       " 'spiders': 294,\n",
       " 'both': 295,\n",
       " 'everything': 296,\n",
       " 'books': 297,\n",
       " 'snow': 298,\n",
       " 'soon': 299,\n",
       " 'small': 300,\n",
       " 'english': 301,\n",
       " 'act': 302,\n",
       " 'seems': 303,\n",
       " 'hope': 304,\n",
       " 'studying': 305,\n",
       " 'question': 306,\n",
       " 'friend': 307,\n",
       " 'says': 308,\n",
       " 'pretty': 309,\n",
       " 'party': 310,\n",
       " 'little': 311,\n",
       " 'married': 312,\n",
       " 'christmas': 313,\n",
       " 'woman': 314,\n",
       " 'assure': 315,\n",
       " 'sit': 316,\n",
       " 'cried': 317,\n",
       " 'turn': 318,\n",
       " 'watch': 319,\n",
       " 'ask': 320,\n",
       " 'die': 321,\n",
       " 'over': 322,\n",
       " 'looked': 323,\n",
       " 'inside': 324,\n",
       " 'addicted': 325,\n",
       " 'reading': 326,\n",
       " 'singing': 327,\n",
       " 'loves': 328,\n",
       " 'might': 329,\n",
       " 'write': 330,\n",
       " 'taught': 331,\n",
       " 'being': 332,\n",
       " 'addict': 333,\n",
       " 'later': 334,\n",
       " 'must': 335,\n",
       " 'seen': 336,\n",
       " 'enough': 337,\n",
       " 'boat': 338,\n",
       " 'phone': 339,\n",
       " 'wonder': 340,\n",
       " 'tonight': 341,\n",
       " 'cost': 342,\n",
       " 'daughter': 343,\n",
       " 'students': 344,\n",
       " 'difficult': 345,\n",
       " 'apartment': 346,\n",
       " \"there's\": 347,\n",
       " 'police': 348,\n",
       " 'few': 349,\n",
       " 'won': 350,\n",
       " 'call': 351,\n",
       " 'lost': 352,\n",
       " 'fun': 353,\n",
       " 'smiled': 354,\n",
       " 'laughed': 355,\n",
       " 'wine': 356,\n",
       " 'nice': 357,\n",
       " 'bad': 358,\n",
       " 'mind': 359,\n",
       " 'mine': 360,\n",
       " 'great': 361,\n",
       " 'caught': 362,\n",
       " 'stupid': 363,\n",
       " 'horse': 364,\n",
       " 'sleepy': 365,\n",
       " 'born': 366,\n",
       " 'rest': 367,\n",
       " 'expect': 368,\n",
       " 'tired': 369,\n",
       " 'big': 370,\n",
       " 'already': 371,\n",
       " 'child': 372,\n",
       " 'interested': 373,\n",
       " 'acting': 374,\n",
       " 'word': 375,\n",
       " 'getting': 376,\n",
       " 'canadian': 377,\n",
       " 'eggs': 378,\n",
       " 'thinks': 379,\n",
       " 'children': 380,\n",
       " 'dangerous': 381,\n",
       " 'four': 382,\n",
       " 'brother': 383,\n",
       " 'singer': 384,\n",
       " 'high': 385,\n",
       " 'point': 386,\n",
       " 'under': 387,\n",
       " 'spend': 388,\n",
       " 'ridiculous': 389,\n",
       " 'swimming': 390,\n",
       " 'listen': 391,\n",
       " 'away': 392,\n",
       " 'lied': 393,\n",
       " 'ready': 394,\n",
       " 'hold': 395,\n",
       " 'care': 396,\n",
       " 'careful': 397,\n",
       " 'ok': 398,\n",
       " 'quiet': 399,\n",
       " 'called': 400,\n",
       " 'hurt': 401,\n",
       " 'cry': 402,\n",
       " 'serious': 403,\n",
       " 'around': 404,\n",
       " 'crying': 405,\n",
       " 'kissed': 406,\n",
       " 'anybody': 407,\n",
       " 'them': 408,\n",
       " 'move': 409,\n",
       " 'finished': 410,\n",
       " 'flowers': 411,\n",
       " 'hear': 412,\n",
       " 'carefully': 413,\n",
       " 'drink': 414,\n",
       " 'mistake': 415,\n",
       " 'happened': 416,\n",
       " 'manager': 417,\n",
       " 'bit': 418,\n",
       " 'seemed': 419,\n",
       " 'win': 420,\n",
       " 'easy': 421,\n",
       " 'acted': 422,\n",
       " 'mean': 423,\n",
       " 'past': 424,\n",
       " 'gone': 425,\n",
       " 'almost': 426,\n",
       " 'guitar': 427,\n",
       " 'best': 428,\n",
       " 'decided': 429,\n",
       " 'equals': 430,\n",
       " 'hat': 431,\n",
       " 'thing': 432,\n",
       " 'important': 433,\n",
       " 'took': 434,\n",
       " 'sitting': 435,\n",
       " 'able': 436,\n",
       " 'blue': 437,\n",
       " 'five': 438,\n",
       " 'matter': 439,\n",
       " 'meat': 440,\n",
       " 'grandfather': 441,\n",
       " 'six': 442,\n",
       " 'times': 443,\n",
       " 'probably': 444,\n",
       " 'john': 445,\n",
       " 'thanks': 446,\n",
       " 'welcome': 447,\n",
       " 'hurts': 448,\n",
       " 'trust': 449,\n",
       " 'quick': 450,\n",
       " 'forgive': 451,\n",
       " 'tall': 452,\n",
       " 'envy': 453,\n",
       " 'boys': 454,\n",
       " 'nervous': 455,\n",
       " 'walk': 456,\n",
       " 'looking': 457,\n",
       " 'questions': 458,\n",
       " 'depressed': 459,\n",
       " 'dreams': 460,\n",
       " 'interesting': 461,\n",
       " 'disappeared': 462,\n",
       " 'full': 463,\n",
       " 'safe': 464,\n",
       " 'spider': 465,\n",
       " 'kind': 466,\n",
       " 'real': 467,\n",
       " 'even': 468,\n",
       " 'music': 469,\n",
       " 'killed': 470,\n",
       " 'changed': 471,\n",
       " 'advice': 472,\n",
       " 'kid': 473,\n",
       " 'worried': 474,\n",
       " 'eating': 475,\n",
       " 'yours': 476,\n",
       " 'attempted': 477,\n",
       " 'coming': 478,\n",
       " 'longer': 479,\n",
       " 'weird': 480,\n",
       " 'addictive': 481,\n",
       " 'song': 482,\n",
       " 'visit': 483,\n",
       " 'surprised': 484,\n",
       " 'unknown': 485,\n",
       " 'name': 486,\n",
       " 'swim': 487,\n",
       " 'taking': 488,\n",
       " 'early': 489,\n",
       " 'country': 490,\n",
       " 'injured': 491,\n",
       " 'put': 492,\n",
       " 'prison': 493,\n",
       " 'gave': 494,\n",
       " 'equal': 495,\n",
       " 'likes': 496,\n",
       " 'speaks': 497,\n",
       " 'named': 498,\n",
       " 'truth': 499,\n",
       " 'station': 500,\n",
       " 'those': 501,\n",
       " 'boy': 502,\n",
       " 'lived': 503,\n",
       " 'parents': 504,\n",
       " 'living': 505,\n",
       " 'chance': 506,\n",
       " 'piano': 507,\n",
       " 'lives': 508,\n",
       " 'whether': 509,\n",
       " 'concentrating': 510,\n",
       " 'success': 511,\n",
       " 'worth': 512,\n",
       " 'summer': 513,\n",
       " 'possible': 514,\n",
       " 'south': 515,\n",
       " 'vacation': 516,\n",
       " 'except': 517,\n",
       " 'things': 518,\n",
       " 'earth': 519,\n",
       " 'run': 520,\n",
       " 'fire': 521,\n",
       " 'jump': 522,\n",
       " 'fair': 523,\n",
       " 'sad': 524,\n",
       " 'show': 525,\n",
       " 'grab': 526,\n",
       " 'cute': 527,\n",
       " 'stand': 528,\n",
       " 'quit': 529,\n",
       " 'dogs': 530,\n",
       " 'check': 531,\n",
       " 'slowly': 532,\n",
       " 'exists': 533,\n",
       " 'chuckled': 534,\n",
       " 'ignore': 535,\n",
       " 'lying': 536,\n",
       " 'strange': 537,\n",
       " 'overslept': 538,\n",
       " 'escaped': 539,\n",
       " 'survived': 540,\n",
       " 'horrible': 541,\n",
       " 'liars': 542,\n",
       " 'driving': 543,\n",
       " 'smoking': 544,\n",
       " 'horses': 545,\n",
       " 'winter': 546,\n",
       " 'honest': 547,\n",
       " 'peace': 548,\n",
       " 'autumn': 549,\n",
       " 'listening': 550,\n",
       " 'melted': 551,\n",
       " 'change': 552,\n",
       " 'smart': 553,\n",
       " 'scared': 554,\n",
       " 'green': 555,\n",
       " 'number': 556,\n",
       " 'hair': 557,\n",
       " 'missed': 558,\n",
       " 'bus': 559,\n",
       " 'plan': 560,\n",
       " 'eyes': 561,\n",
       " 'laugh': 562,\n",
       " 'waiting': 563,\n",
       " 'idea': 564,\n",
       " 'girl': 565,\n",
       " 'extremely': 566,\n",
       " 'save': 567,\n",
       " 'stopped': 568,\n",
       " '2013': 569,\n",
       " 'light': 570,\n",
       " 'sick': 571,\n",
       " 'monday': 572,\n",
       " 'white': 573,\n",
       " 'close': 574,\n",
       " 'africa': 575,\n",
       " 'abroad': 576,\n",
       " 'tennis': 577,\n",
       " 'passport': 578,\n",
       " 'weather': 579,\n",
       " 'suffer': 580,\n",
       " 'began': 581,\n",
       " 'allowed': 582,\n",
       " 'color': 583,\n",
       " 'dinner': 584,\n",
       " 'sometimes': 585,\n",
       " 'plus': 586,\n",
       " 'turned': 587,\n",
       " 'catch': 588,\n",
       " 'morning': 589,\n",
       " 'waste': 590,\n",
       " 'discount': 591,\n",
       " 'delicious': 592,\n",
       " 'prefer': 593,\n",
       " 'completely': 594,\n",
       " 'men': 595,\n",
       " 'suffering': 596,\n",
       " 'animals': 597,\n",
       " 'body': 598,\n",
       " 'diagnosed': 599,\n",
       " 'until': 600,\n",
       " 'humans': 601,\n",
       " 'needs': 602,\n",
       " 'crazy': 603,\n",
       " 'clean': 604,\n",
       " 'first': 605,\n",
       " 'accident': 606,\n",
       " 'watching': 607,\n",
       " 'dress': 608,\n",
       " 'family': 609,\n",
       " 'bigger': 610,\n",
       " 'consists': 611,\n",
       " 'viruses': 612,\n",
       " 'doctor': 613,\n",
       " 'wife': 614,\n",
       " 'bottle': 615,\n",
       " 'spent': 616,\n",
       " 'which': 617,\n",
       " 'studied': 618,\n",
       " 'final': 619,\n",
       " 'bottom': 620,\n",
       " 'drinks': 621,\n",
       " 'japan': 622,\n",
       " 'brain': 623,\n",
       " 'autism': 624,\n",
       " 'alice': 625,\n",
       " 'since': 626,\n",
       " 'hello': 627,\n",
       " 'oh': 628,\n",
       " 'relax': 629,\n",
       " 'shoot': 630,\n",
       " 'smile': 631,\n",
       " 'awesome': 632,\n",
       " 'perfect': 633,\n",
       " 'shut': 634,\n",
       " 'deep': 635,\n",
       " 'hurry': 636,\n",
       " 'calm': 637,\n",
       " 'follow': 638,\n",
       " 'fainted': 639,\n",
       " 'rained': 640,\n",
       " 'seriously': 641,\n",
       " 'thank': 642,\n",
       " 'talked': 643,\n",
       " 'waited': 644,\n",
       " 'wonderful': 645,\n",
       " 'patient': 646,\n",
       " 'carry': 647,\n",
       " 'along': 648,\n",
       " 'definitely': 649,\n",
       " 'warm': 650,\n",
       " 'danced': 651,\n",
       " 'failed': 652,\n",
       " 'helped': 653,\n",
       " 'finish': 654,\n",
       " 'moving': 655,\n",
       " 'talks': 656,\n",
       " 'grow': 657,\n",
       " 'coughed': 658,\n",
       " 'giggled': 659,\n",
       " 'quickly': 660,\n",
       " 'exist': 661,\n",
       " 'succeeded': 662,\n",
       " 'hot': 663,\n",
       " 'dancing': 664,\n",
       " 'smiling': 665,\n",
       " 'refused': 666,\n",
       " 'engaged': 667,\n",
       " 'fighting': 668,\n",
       " 'head': 669,\n",
       " 'worrying': 670,\n",
       " 'screamed': 671,\n",
       " 'hesitated': 672,\n",
       " 'apples': 673,\n",
       " 'korean': 674,\n",
       " 'volunteered': 675,\n",
       " 'fascinating': 676,\n",
       " 'voice': 677,\n",
       " 'pencil': 678,\n",
       " 'lazy': 679,\n",
       " 'sleeping': 680,\n",
       " 'free': 681,\n",
       " 'fast': 682,\n",
       " 'liar': 683,\n",
       " 'regret': 684,\n",
       " 'champagne': 685,\n",
       " 'mother': 686,\n",
       " 'doubted': 687,\n",
       " 'kicked': 688,\n",
       " 'outside': 689,\n",
       " 'sent': 690,\n",
       " 'poor': 691,\n",
       " 'watched': 692,\n",
       " 'movie': 693,\n",
       " 'shy': 694,\n",
       " 'short': 695,\n",
       " 'health': 696,\n",
       " 'minute': 697,\n",
       " 'shoes': 698,\n",
       " 'virus': 699,\n",
       " 'timid': 700,\n",
       " 'guy': 701,\n",
       " 'death': 702,\n",
       " 'either': 703,\n",
       " 'hide': 704,\n",
       " 'funeral': 705,\n",
       " 'useful': 706,\n",
       " 'single': 707,\n",
       " 'committed': 708,\n",
       " 'library': 709,\n",
       " 'train': 710,\n",
       " 'rarely': 711,\n",
       " 'worked': 712,\n",
       " 'drug': 713,\n",
       " 'playing': 714,\n",
       " 'marry': 715,\n",
       " 'ought': 716,\n",
       " 'women': 717,\n",
       " 'dreamed': 718,\n",
       " 'shirt': 719,\n",
       " 'melt': 720,\n",
       " 'learns': 721,\n",
       " 'motivated': 722,\n",
       " 'own': 723,\n",
       " 'popular': 724,\n",
       " 'borrow': 725,\n",
       " 'store': 726,\n",
       " 'feeling': 727,\n",
       " 'seeing': 728,\n",
       " 'group': 729,\n",
       " 'somewhere': 730,\n",
       " 'happening': 731,\n",
       " 'while': 732,\n",
       " 'sign': 733,\n",
       " 'skills': 734,\n",
       " 'sat': 735,\n",
       " 'physics': 736,\n",
       " 'cell': 737,\n",
       " 'drunk': 738,\n",
       " 'fired': 739,\n",
       " 'business': 740,\n",
       " 'media': 741,\n",
       " 'weight': 742,\n",
       " 'baby': 743,\n",
       " 'carried': 744,\n",
       " 'responsibility': 745,\n",
       " 'spring': 746,\n",
       " 'feelings': 747,\n",
       " 'thin': 748,\n",
       " 'sold': 749,\n",
       " 'exactly': 750,\n",
       " 'bread': 751,\n",
       " 'eaten': 752,\n",
       " 'door': 753,\n",
       " 'forgotten': 754,\n",
       " 'joke': 755,\n",
       " 'brown': 756,\n",
       " 'professor': 757,\n",
       " 'heroin': 758,\n",
       " 'unhappy': 759,\n",
       " 'park': 760,\n",
       " 'breakfast': 761,\n",
       " 'pay': 762,\n",
       " 'code': 763,\n",
       " 'exam': 764,\n",
       " 'low': 765,\n",
       " 'pressure': 766,\n",
       " 'ashamed': 767,\n",
       " 'coat': 768,\n",
       " 'suffers': 769,\n",
       " 'war': 770,\n",
       " 'present': 771,\n",
       " 'price': 772,\n",
       " 'loved': 773,\n",
       " 'broke': 774,\n",
       " 'watering': 775,\n",
       " 'jeans': 776,\n",
       " 'walking': 777,\n",
       " \"everyone's\": 778,\n",
       " 'faith': 779,\n",
       " 'unshaken': 780,\n",
       " 'test': 781,\n",
       " 'several': 782,\n",
       " 'news': 783,\n",
       " 'hut': 784,\n",
       " 'maintained': 785,\n",
       " 'depression': 786,\n",
       " 'doctors': 787,\n",
       " 'sooner': 788,\n",
       " 'earlier': 789,\n",
       " 'air': 790,\n",
       " 'law': 791,\n",
       " 'avoid': 792,\n",
       " 'hospital': 793,\n",
       " 'pry': 794,\n",
       " 'private': 795,\n",
       " 'twenty': 796,\n",
       " 'native': 797,\n",
       " 'speaker': 798,\n",
       " 'branch': 799,\n",
       " 'engineer': 800,\n",
       " 'arguing': 801,\n",
       " 'whole': 802,\n",
       " 'common': 803,\n",
       " 'become': 804,\n",
       " 'key': 805,\n",
       " 'speaking': 806,\n",
       " 'place': 807,\n",
       " 'proud': 808,\n",
       " 'words': 809,\n",
       " 'story': 810,\n",
       " 'cause': 811,\n",
       " 'destination': 812,\n",
       " 'united': 813,\n",
       " 'imagine': 814,\n",
       " 'beings': 815,\n",
       " 'necessary': 816,\n",
       " 'foreign': 817,\n",
       " 'cheese': 818,\n",
       " 'player': 819,\n",
       " 'large': 820,\n",
       " 'actually': 821,\n",
       " 'each': 822,\n",
       " 'pass': 823,\n",
       " 'teeth': 824,\n",
       " 'tend': 825,\n",
       " 'fight': 826,\n",
       " 'service': 827,\n",
       " 'airport': 828,\n",
       " 'month': 829,\n",
       " 'days': 830,\n",
       " 'trouble': 831,\n",
       " 'syndrome': 832,\n",
       " 'repeat': 833,\n",
       " 'less': 834,\n",
       " 'difference': 835,\n",
       " 'afternoon': 836,\n",
       " 'between': 837,\n",
       " 'stuck': 838,\n",
       " 'intelligent': 839,\n",
       " 'difficulty': 840,\n",
       " 'graduate': 841,\n",
       " 'minutes': 842,\n",
       " 'convince': 843,\n",
       " 'such': 844,\n",
       " 'view': 845,\n",
       " 'begin': 846,\n",
       " 'attack': 847,\n",
       " 'goodbye': 848,\n",
       " 'hit': 849,\n",
       " 'agree': 850,\n",
       " 'wake': 851,\n",
       " 'cool': 852,\n",
       " 'ahead': 853,\n",
       " 'forgot': 854,\n",
       " 'birds': 855,\n",
       " 'fly': 856,\n",
       " 'fantastic': 857,\n",
       " 'luck': 858,\n",
       " 'awful': 859,\n",
       " 'promise': 860,\n",
       " 'stinks': 861,\n",
       " 'course': 862,\n",
       " 'then': 863,\n",
       " 'aboard': 864,\n",
       " 'bring': 865,\n",
       " 'choose': 866,\n",
       " 'face': 867,\n",
       " 'facts': 868,\n",
       " 'tragic': 869,\n",
       " 'mama': 870,\n",
       " 'once': 871,\n",
       " 'nodded': 872,\n",
       " 'decide': 873,\n",
       " 'shocked': 874,\n",
       " 'timing': 875,\n",
       " 'plants': 876,\n",
       " 'release': 877,\n",
       " 'cheated': 878,\n",
       " 'clapped': 879,\n",
       " 'drowned': 880,\n",
       " 'exhaled': 881,\n",
       " 'grinned': 882,\n",
       " 'forward': 883,\n",
       " 'faster': 884,\n",
       " 'examine': 885,\n",
       " 'annoying': 886,\n",
       " 'arrogant': 887,\n",
       " 'apologized': 888,\n",
       " 'digging': 889,\n",
       " 'grumbled': 890,\n",
       " 'silly': 891,\n",
       " 'sober': 892,\n",
       " 'listened': 893,\n",
       " 'unbelievable': 894,\n",
       " 'apologize': 895,\n",
       " 'dies': 896,\n",
       " 'guilty': 897,\n",
       " 'miss': 898,\n",
       " 'smell': 899,\n",
       " 'kill': 900,\n",
       " 'naive': 901,\n",
       " 'confessed': 902,\n",
       " 'closely': 903,\n",
       " 'stood': 904,\n",
       " 'funny': 905,\n",
       " 'simple': 906,\n",
       " 'searching': 907,\n",
       " 'ice': 908,\n",
       " 'hates': 909,\n",
       " 'surrendered': 910,\n",
       " 'rap': 911,\n",
       " 'prayed': 912,\n",
       " 'trains': 913,\n",
       " 'excluded': 914,\n",
       " 'embarrassed': 915,\n",
       " 'black': 916,\n",
       " 'continue': 917,\n",
       " 'boss': 918,\n",
       " 'bag': 919,\n",
       " 'immediately': 920,\n",
       " 'changes': 921,\n",
       " 'dead': 922,\n",
       " 'somebody': 923,\n",
       " 'flower': 924,\n",
       " 'proof': 925,\n",
       " 'exercise': 926,\n",
       " 'bless': 927,\n",
       " 'seven': 928,\n",
       " 'stole': 929,\n",
       " 'charge': 930,\n",
       " 'clock': 931,\n",
       " 'match': 932,\n",
       " 'kept': 933,\n",
       " 'delusion': 934,\n",
       " 'return': 935,\n",
       " \"asperger's\": 936,\n",
       " 'weak': 937,\n",
       " 'whose': 938,\n",
       " 'older': 939,\n",
       " 'cooking': 940,\n",
       " \"everything's\": 941,\n",
       " 'normal': 942,\n",
       " 'fear': 943,\n",
       " 'reservations': 944,\n",
       " 'wheel': 945,\n",
       " 'tv': 946,\n",
       " 'fault': 947,\n",
       " 'box': 948,\n",
       " 'relaxing': 949,\n",
       " 'winning': 950,\n",
       " 'closed': 951,\n",
       " 'message': 952,\n",
       " 'scare': 953,\n",
       " 'isolated': 954,\n",
       " 'major': 955,\n",
       " 'student': 956,\n",
       " 'cocaine': 957,\n",
       " 'snows': 958,\n",
       " 'mom': 959,\n",
       " 'camera': 960,\n",
       " 'acts': 961,\n",
       " 'hungry': 962,\n",
       " 'shaking': 963,\n",
       " 'lose': 964,\n",
       " 'cuisine': 965,\n",
       " 'helping': 966,\n",
       " 'others': 967,\n",
       " '230': 968,\n",
       " 'angry': 969,\n",
       " 'its': 970,\n",
       " 'lawyers': 971,\n",
       " 'son': 972,\n",
       " 'choice': 973,\n",
       " 'heart': 974,\n",
       " 'muscle': 975,\n",
       " 'dropped': 976,\n",
       " 'knife': 977,\n",
       " 'coffee': 978,\n",
       " 'window': 979,\n",
       " 'hated': 980,\n",
       " 'majors': 981,\n",
       " 'terrible': 982,\n",
       " 'also': 983,\n",
       " 'obviously': 984,\n",
       " 'opened': 985,\n",
       " 'deeply': 986,\n",
       " 'causes': 987,\n",
       " 'loneliness': 988,\n",
       " 'following': 989,\n",
       " 'insects': 990,\n",
       " 'pictures': 991,\n",
       " 'invited': 992,\n",
       " 'disappointed': 993,\n",
       " 'reply': 994,\n",
       " 'kick': 995,\n",
       " 'photo': 996,\n",
       " 'husband': 997,\n",
       " 'wedding': 998,\n",
       " 'keys': 999,\n",
       " 'sun': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 1,\n",
       " '<end>': 2,\n",
       " '톰은': 3,\n",
       " '있어': 4,\n",
       " '난': 5,\n",
       " '톰이': 6,\n",
       " '나는': 7,\n",
       " '그': 8,\n",
       " '내가': 9,\n",
       " '수': 10,\n",
       " '내': 11,\n",
       " '이': 12,\n",
       " '것': 13,\n",
       " '네가': 14,\n",
       " '같아': 15,\n",
       " '않아': 16,\n",
       " '거야': 17,\n",
       " '너': 18,\n",
       " '없어': 19,\n",
       " '더': 20,\n",
       " '걸': 21,\n",
       " '할': 22,\n",
       " '안': 23,\n",
       " '좀': 24,\n",
       " '것을': 25,\n",
       " '해': 26,\n",
       " '그는': 27,\n",
       " '왜': 28,\n",
       " '이건': 29,\n",
       " '적': 30,\n",
       " '너무': 31,\n",
       " '있는': 32,\n",
       " '우린': 33,\n",
       " '알고': 34,\n",
       " '정말': 35,\n",
       " '한': 36,\n",
       " '프랑스어를': 37,\n",
       " '사람은': 38,\n",
       " '우리는': 39,\n",
       " '톰을': 40,\n",
       " '가장': 41,\n",
       " '있었어': 42,\n",
       " '네': 43,\n",
       " '아주': 44,\n",
       " '그걸': 45,\n",
       " '했어': 46,\n",
       " '싶어': 47,\n",
       " '있다': 48,\n",
       " '줄': 49,\n",
       " '잘': 50,\n",
       " '나': 51,\n",
       " '계속': 52,\n",
       " '넌': 53,\n",
       " '좋아해': 54,\n",
       " '건': 55,\n",
       " '날': 56,\n",
       " '하고': 57,\n",
       " '자기': 58,\n",
       " '아니야': 59,\n",
       " '이게': 60,\n",
       " '우리': 61,\n",
       " '것은': 62,\n",
       " '좋아하는': 63,\n",
       " '않았어': 64,\n",
       " '생각해': 65,\n",
       " '아직도': 66,\n",
       " '얼마나': 67,\n",
       " '톰한테': 68,\n",
       " '하지': 69,\n",
       " '거짓말': 70,\n",
       " '마': 71,\n",
       " '있을': 72,\n",
       " '사람들은': 73,\n",
       " '톰의': 74,\n",
       " '아직': 75,\n",
       " '법을': 76,\n",
       " '모두': 77,\n",
       " '돼': 78,\n",
       " '많이': 79,\n",
       " '적이': 80,\n",
       " '메리가': 81,\n",
       " '와': 82,\n",
       " '미안해': 83,\n",
       " '그만': 84,\n",
       " '모든': 85,\n",
       " '메리는': 86,\n",
       " '말해': 87,\n",
       " '누가': 88,\n",
       " '못': 89,\n",
       " '톰에게': 90,\n",
       " '웃었어': 91,\n",
       " '좋은': 92,\n",
       " '그렇게': 93,\n",
       " '톰과': 94,\n",
       " '하는': 95,\n",
       " '톰이랑': 96,\n",
       " '어떤': 97,\n",
       " '우리가': 98,\n",
       " '집에': 99,\n",
       " '뭔가': 100,\n",
       " '말을': 101,\n",
       " '너는': 102,\n",
       " '꽤': 103,\n",
       " '톰': 104,\n",
       " '무슨': 105,\n",
       " '프랑스어': 106,\n",
       " '몰랐어': 107,\n",
       " '여기': 108,\n",
       " '이걸': 109,\n",
       " '여기에': 110,\n",
       " '배웠어': 111,\n",
       " '나한테': 112,\n",
       " '싶지': 113,\n",
       " '갈': 114,\n",
       " '했다': 115,\n",
       " '어떻게': 116,\n",
       " '많은': 117,\n",
       " '했는데': 118,\n",
       " '때': 119,\n",
       " '거기에': 120,\n",
       " '가': 121,\n",
       " '사람이': 122,\n",
       " '죽었어': 123,\n",
       " '이거': 124,\n",
       " '지금': 125,\n",
       " '아무도': 126,\n",
       " '있었다': 127,\n",
       " '책을': 128,\n",
       " '보여': 129,\n",
       " '어제': 130,\n",
       " '같이': 131,\n",
       " '내일': 132,\n",
       " '학교에': 133,\n",
       " '너한테': 134,\n",
       " '그녀는': 135,\n",
       " '말했어': 136,\n",
       " '다': 137,\n",
       " '몇': 138,\n",
       " '알아': 139,\n",
       " '절대': 140,\n",
       " '왔어': 141,\n",
       " '거기': 142,\n",
       " '그건': 143,\n",
       " '좋아': 144,\n",
       " '거': 145,\n",
       " '진짜': 146,\n",
       " '것이': 147,\n",
       " '게': 148,\n",
       " '사람이야': 149,\n",
       " '나이에': 150,\n",
       " '비해': 151,\n",
       " '열심히': 152,\n",
       " '봐': 153,\n",
       " '이제': 154,\n",
       " '있어요': 155,\n",
       " '있니': 156,\n",
       " '않을': 157,\n",
       " '본': 158,\n",
       " '그의': 159,\n",
       " '널': 160,\n",
       " '좋겠어': 161,\n",
       " '배우고': 162,\n",
       " '전에': 163,\n",
       " '다시': 164,\n",
       " '빨리': 165,\n",
       " '줘': 166,\n",
       " '있어봐': 167,\n",
       " '누군가': 168,\n",
       " '사람': 169,\n",
       " '뭐야': 170,\n",
       " '싫어': 171,\n",
       " '몰라': 172,\n",
       " '주세요': 173,\n",
       " '고양이를': 174,\n",
       " '거의': 175,\n",
       " '그것을': 176,\n",
       " '큰': 177,\n",
       " '살고': 178,\n",
       " '프랑스': 179,\n",
       " '이상': 180,\n",
       " '있지': 181,\n",
       " '항상': 182,\n",
       " '탐은': 183,\n",
       " '싶은': 184,\n",
       " '호주에': 185,\n",
       " '알았어': 186,\n",
       " '들어': 187,\n",
       " '이렇게': 188,\n",
       " '수가': 189,\n",
       " '가고': 190,\n",
       " '자신이': 191,\n",
       " '모두들': 192,\n",
       " '봤어': 193,\n",
       " '샀어': 194,\n",
       " '언어를': 195,\n",
       " '없었어': 196,\n",
       " '않는다': 197,\n",
       " '위해': 198,\n",
       " '시간을': 199,\n",
       " '가지고': 200,\n",
       " '같은': 201,\n",
       " '프랑스어로': 202,\n",
       " '프랑스어는': 203,\n",
       " '말았어야': 204,\n",
       " '없이': 205,\n",
       " '대해': 206,\n",
       " '될': 207,\n",
       " '거라고': 208,\n",
       " '가본': 209,\n",
       " '우리한테': 210,\n",
       " '앉아': 211,\n",
       " '그들이': 212,\n",
       " '사람들이': 213,\n",
       " '눈이': 214,\n",
       " '여기서': 215,\n",
       " '이렇게나': 216,\n",
       " '그냥': 217,\n",
       " '안에': 218,\n",
       " '고양이는': 219,\n",
       " '일을': 220,\n",
       " '모두가': 221,\n",
       " '말': 222,\n",
       " '둘': 223,\n",
       " '말했다': 224,\n",
       " '언제': 225,\n",
       " '한때': 226,\n",
       " '도와줄': 227,\n",
       " '그게': 228,\n",
       " '혼자': 229,\n",
       " '우리의': 230,\n",
       " '바로': 231,\n",
       " '살아': 232,\n",
       " '어디서': 233,\n",
       " '시간이': 234,\n",
       " '입고': 235,\n",
       " '보스턴에': 236,\n",
       " '가지': 237,\n",
       " '그가': 238,\n",
       " '조심해': 239,\n",
       " '제발': 240,\n",
       " '키가': 241,\n",
       " '보고': 242,\n",
       " '것이다': 243,\n",
       " '물': 244,\n",
       " '꿈을': 245,\n",
       " '나를': 246,\n",
       " '곧': 247,\n",
       " '일찍': 248,\n",
       " '한번이라도': 249,\n",
       " '컴퓨터를': 250,\n",
       " '세': 251,\n",
       " '필요가': 252,\n",
       " '그다지': 253,\n",
       " '메리의': 254,\n",
       " '공부하는': 255,\n",
       " '아는': 256,\n",
       " '생각하지': 257,\n",
       " '훨씬': 258,\n",
       " '일해': 259,\n",
       " '나도': 260,\n",
       " '마세요': 261,\n",
       " '저걸': 262,\n",
       " '서로': 263,\n",
       " '사람을': 264,\n",
       " '말이': 265,\n",
       " '돈이': 266,\n",
       " '중독자야': 267,\n",
       " '외로워': 268,\n",
       " '고양이가': 269,\n",
       " '살': 270,\n",
       " '한다': 271,\n",
       " '좋아해요': 272,\n",
       " '아냐': 273,\n",
       " '톰도': 274,\n",
       " '사람의': 275,\n",
       " '들었어': 276,\n",
       " '보냈어': 277,\n",
       " '뿐이야': 278,\n",
       " '너희': 279,\n",
       " '어디에': 280,\n",
       " '거라는': 281,\n",
       " '좋아하지': 282,\n",
       " '고양이': 283,\n",
       " '자기가': 284,\n",
       " '아마': 285,\n",
       " '명': 286,\n",
       " '가는': 287,\n",
       " '가끔': 288,\n",
       " '있다는': 289,\n",
       " '오늘': 290,\n",
       " '모자를': 291,\n",
       " '있다고': 292,\n",
       " '배우는': 293,\n",
       " '않은데': 294,\n",
       " '척': 295,\n",
       " '다음': 296,\n",
       " '없는': 297,\n",
       " '쓰고': 298,\n",
       " '아름다운': 299,\n",
       " '이겼어': 300,\n",
       " '고마워': 301,\n",
       " '떠났어': 302,\n",
       " '커': 303,\n",
       " '괜찮은': 304,\n",
       " '조용히': 305,\n",
       " '쳐': 306,\n",
       " '그거': 307,\n",
       " '누구나': 308,\n",
       " '너의': 309,\n",
       " '저는': 310,\n",
       " '잘해': 311,\n",
       " '보였어': 312,\n",
       " '좋아한다': 313,\n",
       " '이것은': 314,\n",
       " '매우': 315,\n",
       " '없다': 316,\n",
       " '물을': 317,\n",
       " '있길': 318,\n",
       " '좋아요': 319,\n",
       " '노력했어': 320,\n",
       " '꿈은': 321,\n",
       " '것처럼': 322,\n",
       " '해야': 323,\n",
       " '이미': 324,\n",
       " '못해': 325,\n",
       " '메리를': 326,\n",
       " '싶어해': 327,\n",
       " '갖고': 328,\n",
       " '생각했어': 329,\n",
       " '당신의': 330,\n",
       " '정말로': 331,\n",
       " '있어서': 332,\n",
       " '중이야': 333,\n",
       " '먹고': 334,\n",
       " '구성되어': 335,\n",
       " '말하는': 336,\n",
       " '메리에게': 337,\n",
       " '딸은': 338,\n",
       " '중': 339,\n",
       " '보내고': 340,\n",
       " '파티에': 341,\n",
       " '매일': 342,\n",
       " '크리스마스': 343,\n",
       " '있는지': 344,\n",
       " '작은': 345,\n",
       " '모르겠어': 346,\n",
       " '나랑': 347,\n",
       " '메리한테': 348,\n",
       " '결혼했어': 349,\n",
       " '줄은': 350,\n",
       " '없었다': 351,\n",
       " '어려움을': 352,\n",
       " '누구': 353,\n",
       " '진정해': 354,\n",
       " '저리': 355,\n",
       " '거짓말을': 356,\n",
       " '뭐': 357,\n",
       " '바라': 358,\n",
       " '죄송합니다': 359,\n",
       " '비가': 360,\n",
       " '가져': 361,\n",
       " '그들은': 362,\n",
       " '울었어': 363,\n",
       " '번': 364,\n",
       " '하나': 365,\n",
       " '혼자서': 366,\n",
       " '진실을': 367,\n",
       " '저': 368,\n",
       " '일은': 369,\n",
       " '어때': 370,\n",
       " '공부해': 371,\n",
       " '않습니다': 372,\n",
       " '노래': 373,\n",
       " '부탁해': 374,\n",
       " '남아': 375,\n",
       " '썼어': 376,\n",
       " '나의': 377,\n",
       " '새로운': 378,\n",
       " '듣고': 379,\n",
       " '않아요': 380,\n",
       " '걸렸어': 381,\n",
       " '시간': 382,\n",
       " '머리가': 383,\n",
       " '키우고': 384,\n",
       " '제가': 385,\n",
       " '제': 386,\n",
       " '그것은': 387,\n",
       " '싫어해': 388,\n",
       " '감기': 389,\n",
       " '사라졌어': 390,\n",
       " '그녀의': 391,\n",
       " '않다': 392,\n",
       " '은행': 393,\n",
       " '수도': 394,\n",
       " '생각': 395,\n",
       " '뭘': 396,\n",
       " '전부': 397,\n",
       " '눈을': 398,\n",
       " '세게': 399,\n",
       " '일하는': 400,\n",
       " '가르쳤어': 401,\n",
       " '지하철로': 402,\n",
       " '줬어': 403,\n",
       " '인간이': 404,\n",
       " '태어났어': 405,\n",
       " '많아': 406,\n",
       " '영어': 407,\n",
       " '별로': 408,\n",
       " '너에게': 409,\n",
       " '일하고': 410,\n",
       " '싶어하지': 411,\n",
       " '약은': 412,\n",
       " '살이': 413,\n",
       " '있습니까': 414,\n",
       " '배울': 415,\n",
       " '쓰는': 416,\n",
       " '싶었어': 417,\n",
       " '보스턴에서': 418,\n",
       " '메리와': 419,\n",
       " '전혀': 420,\n",
       " '최선을': 421,\n",
       " '캐나다': 422,\n",
       " '시작했어': 423,\n",
       " '되어': 424,\n",
       " '컴퓨터는': 425,\n",
       " '마리가': 426,\n",
       " '동물이야': 427,\n",
       " '조금': 428,\n",
       " '선생님이야': 429,\n",
       " '운전하는': 430,\n",
       " '또': 431,\n",
       " '어려워': 432,\n",
       " '물이': 433,\n",
       " '계란': 434,\n",
       " '저런': 435,\n",
       " '나보다': 436,\n",
       " '정도로': 437,\n",
       " '만난': 438,\n",
       " '시달리고': 439,\n",
       " '있나요': 440,\n",
       " '텐데': 441,\n",
       " '수는': 442,\n",
       " '집을': 443,\n",
       " '소프트웨어': 444,\n",
       " '외로울': 445,\n",
       " '과학': 446,\n",
       " '달리': 447,\n",
       " '생일': 448,\n",
       " '돈을': 449,\n",
       " '중에': 450,\n",
       " '함께': 451,\n",
       " '일이': 452,\n",
       " '거지': 453,\n",
       " '두': 454,\n",
       " '같진': 455,\n",
       " '만약': 456,\n",
       " '사실이야': 457,\n",
       " '경찰은': 458,\n",
       " '데': 459,\n",
       " '것이라고': 460,\n",
       " '명의': 461,\n",
       " '그런': 462,\n",
       " '안녕': 463,\n",
       " '뛰어': 464,\n",
       " '도와줘': 465,\n",
       " '기다려': 466,\n",
       " '시작해': 467,\n",
       " '그만해': 468,\n",
       " '믿어': 469,\n",
       " '당장': 470,\n",
       " '읽어': 471,\n",
       " '쳤어': 472,\n",
       " '해봐': 473,\n",
       " '와인': 474,\n",
       " '말하지': 475,\n",
       " '천천히': 476,\n",
       " '먹어': 477,\n",
       " '괜찮아': 478,\n",
       " '가만히': 479,\n",
       " '기분이': 480,\n",
       " '기억하고': 481,\n",
       " '다쳤어': 482,\n",
       " '늦잠잤어': 483,\n",
       " '물에': 484,\n",
       " '살아남았어': 485,\n",
       " '이해해': 486,\n",
       " '끝냈어': 487,\n",
       " '싶어요': 488,\n",
       " '해줘': 489,\n",
       " '비명을': 490,\n",
       " '좋아합니다': 491,\n",
       " '찾고': 492,\n",
       " '늦어서': 493,\n",
       " '우리에게': 494,\n",
       " '피가': 495,\n",
       " '써': 496,\n",
       " '바빠': 497,\n",
       " '자고': 498,\n",
       " '생선': 499,\n",
       " '무엇을': 500,\n",
       " '뭐가': 501,\n",
       " '하지마': 502,\n",
       " '자동차를': 503,\n",
       " '먹는': 504,\n",
       " '남자야': 505,\n",
       " '내버려': 506,\n",
       " '거미를': 507,\n",
       " '졸려': 508,\n",
       " '빨간색': 509,\n",
       " '학교를': 510,\n",
       " '자주': 511,\n",
       " '생겼어': 512,\n",
       " '방은': 513,\n",
       " '거미가': 514,\n",
       " '소심한': 515,\n",
       " '숨어': 516,\n",
       " '나이가': 517,\n",
       " '있을까': 518,\n",
       " '미안한데': 519,\n",
       " '자살을': 520,\n",
       " '너를': 521,\n",
       " '더이상': 522,\n",
       " '갔어': 523,\n",
       " '몇몇': 524,\n",
       " '얘기해': 525,\n",
       " '만났어': 526,\n",
       " '이상하게': 527,\n",
       " '확실히': 528,\n",
       " '마약': 529,\n",
       " '중독성': 530,\n",
       " '과학이': 531,\n",
       " '궁금하네': 532,\n",
       " '사회적인': 533,\n",
       " '문제야': 534,\n",
       " '단어야': 535,\n",
       " '굴어': 536,\n",
       " '죽을': 537,\n",
       " '여자가': 538,\n",
       " '꿨어': 539,\n",
       " '치는': 540,\n",
       " '있습니다': 541,\n",
       " '배워': 542,\n",
       " '배를': 543,\n",
       " '실수를': 544,\n",
       " '된': 545,\n",
       " '친구를': 546,\n",
       " '갔으면': 547,\n",
       " '기분이야': 548,\n",
       " '곳에': 549,\n",
       " '더하기': 550,\n",
       " '위험한': 551,\n",
       " '것에': 552,\n",
       " '공부해야': 553,\n",
       " '걸릴': 554,\n",
       " '작아': 555,\n",
       " '차를': 556,\n",
       " '대한': 557,\n",
       " '너랑': 558,\n",
       " '관심': 559,\n",
       " '뭔가를': 560,\n",
       " '방법을': 561,\n",
       " '알려': 562,\n",
       " '알게': 563,\n",
       " '그치': 564,\n",
       " '같지': 565,\n",
       " '하려고': 566,\n",
       " '메리': 567,\n",
       " '수영을': 568,\n",
       " '진단받았어': 569,\n",
       " '후에': 570,\n",
       " '문제를': 571,\n",
       " '줘야': 572,\n",
       " '누구야': 573,\n",
       " '가수는': 574,\n",
       " '아파트에서': 575,\n",
       " '너와': 576,\n",
       " '종류의': 577,\n",
       " '산': 578,\n",
       " '됐어': 579,\n",
       " '합니다': 580,\n",
       " '학교에서': 581,\n",
       " '먹을': 582,\n",
       " '빼고': 583,\n",
       " '노래를': 584,\n",
       " '좋겠다': 585,\n",
       " '믿을': 586,\n",
       " '여섯': 587,\n",
       " '질문에': 588,\n",
       " '답할': 589,\n",
       " '만한': 590,\n",
       " '호주에서': 591,\n",
       " '가기로': 592,\n",
       " '다른': 593,\n",
       " '세상에': 594,\n",
       " '않는': 595,\n",
       " '동안': 596,\n",
       " '딱히': 597,\n",
       " '내년에': 598,\n",
       " '이야기를': 599,\n",
       " '원인은': 600,\n",
       " '혼자인': 601,\n",
       " '반에서': 602,\n",
       " '걔는': 603,\n",
       " '한번도': 604,\n",
       " '집중하려고': 605,\n",
       " '오직': 606,\n",
       " '성공을': 607,\n",
       " '니가': 608,\n",
       " '학생이': 609,\n",
       " '있다면': 610,\n",
       " '냉장고에': 611,\n",
       " '얘기': 612,\n",
       " '메리라는': 613,\n",
       " '세상에서': 614,\n",
       " '수백': 615,\n",
       " '언어인': 616,\n",
       " '쏴': 617,\n",
       " '이런': 618,\n",
       " '웃어': 619,\n",
       " '일어나': 620,\n",
       " '들어와': 621,\n",
       " '슬퍼': 622,\n",
       " '안돼': 623,\n",
       " '아파': 624,\n",
       " '가자': 625,\n",
       " '졌어': 626,\n",
       " '여기로': 627,\n",
       " '개가': 628,\n",
       " '잊어버려': 629,\n",
       " '미소지었어': 630,\n",
       " '떠나': 631,\n",
       " '멈춰': 632,\n",
       " '돌아': 633,\n",
       " '기다렸어': 634,\n",
       " '멋져': 635,\n",
       " '용서해': 636,\n",
       " '인간이야': 637,\n",
       " '무시해': 638,\n",
       " '거예요': 639,\n",
       " '소리를': 640,\n",
       " '그를': 641,\n",
       " '아무': 642,\n",
       " '사람들': 643,\n",
       " '빠져': 644,\n",
       " '숨을': 645,\n",
       " '저건': 646,\n",
       " '것입니다': 647,\n",
       " '먹지': 648,\n",
       " '이것들': 649,\n",
       " '성공했어': 650,\n",
       " '거짓말쟁이가': 651,\n",
       " '자살입니다': 652,\n",
       " '노래해': 653,\n",
       " '말하고': 654,\n",
       " '담배': 655,\n",
       " '주장했어': 656,\n",
       " '잠이': 657,\n",
       " '죽고': 658,\n",
       " '우울해': 659,\n",
       " '아파요': 660,\n",
       " '걱정': 661,\n",
       " '고양이야': 662,\n",
       " '질렀어': 663,\n",
       " '평화를': 664,\n",
       " '도와': 665,\n",
       " '집이': 666,\n",
       " '두세요': 667,\n",
       " '과학은': 668,\n",
       " '필요해': 669,\n",
       " '꿔': 670,\n",
       " '기도했어': 671,\n",
       " '감기에': 672,\n",
       " '안다': 673,\n",
       " '될까': 674,\n",
       " '검은색': 675,\n",
       " '마셔': 676,\n",
       " '마리를': 677,\n",
       " '않았다': 678,\n",
       " '없어요': 679,\n",
       " '어딨어': 680,\n",
       " '샴폐인': 681,\n",
       " '일곱': 682,\n",
       " '갈게': 683,\n",
       " '극히': 684,\n",
       " '누구도': 685,\n",
       " '무언가': 686,\n",
       " '거짓말쟁이다': 687,\n",
       " '어디': 688,\n",
       " '조언을': 689,\n",
       " '이해가': 690,\n",
       " '이길': 691,\n",
       " '않으려고': 692,\n",
       " '2013년에': 693,\n",
       " '만들었어': 694,\n",
       " '전': 695,\n",
       " '두렵지': 696,\n",
       " '나에게': 697,\n",
       " '충분히': 698,\n",
       " '받았어': 699,\n",
       " '가르쳐': 700,\n",
       " '거미': 701,\n",
       " '무서워해': 702,\n",
       " '시도했다': 703,\n",
       " '자살했다': 704,\n",
       " '기다리고': 705,\n",
       " '행동해': 706,\n",
       " '자신을': 707,\n",
       " '과거를': 708,\n",
       " '그사람이': 709,\n",
       " '도움을': 710,\n",
       " '한국에': 711,\n",
       " '싶다': 712,\n",
       " '오늘은': 713,\n",
       " '30분에': 714,\n",
       " '도움이': 715,\n",
       " '될거야': 716,\n",
       " '친구야': 717,\n",
       " '개를': 718,\n",
       " '떠나야': 719,\n",
       " '뭐라고': 720,\n",
       " '여권': 721,\n",
       " '학교': 722,\n",
       " '하겠어': 723,\n",
       " '훌륭한': 724,\n",
       " '있어도': 725,\n",
       " '말하길': 726,\n",
       " '탔다': 727,\n",
       " '이름은': 728,\n",
       " '인기가': 729,\n",
       " '개': 730,\n",
       " '하도록': 731,\n",
       " '해본': 732,\n",
       " '종종': 733,\n",
       " '도울': 734,\n",
       " '너만큼': 735,\n",
       " '차': 736,\n",
       " '뭔가가': 737,\n",
       " '일이야': 738,\n",
       " '보인다': 739,\n",
       " '속에': 740,\n",
       " '현재': 741,\n",
       " '싶니': 742,\n",
       " '대로': 743,\n",
       " '감기가': 744,\n",
       " '안나아': 745,\n",
       " '단지': 746,\n",
       " '게으른': 747,\n",
       " '감옥에': 748,\n",
       " '중이었어': 749,\n",
       " '올': 750,\n",
       " '아빠는': 751,\n",
       " '녹아': 752,\n",
       " '나쁜': 753,\n",
       " '침대에서': 754,\n",
       " '어려': 755,\n",
       " '정확히': 756,\n",
       " '믿고': 757,\n",
       " '가격은': 758,\n",
       " '중요한': 759,\n",
       " '과학에': 760,\n",
       " '갈색': 761,\n",
       " '교수야': 762,\n",
       " '애들은': 763,\n",
       " '책은': 764,\n",
       " '마치': 765,\n",
       " '힘든': 766,\n",
       " '읽고': 767,\n",
       " '침대에': 768,\n",
       " '대체': 769,\n",
       " '완전히': 770,\n",
       " '그에게': 771,\n",
       " '나중에': 772,\n",
       " '한번': 773,\n",
       " '간': 774,\n",
       " '작년에': 775,\n",
       " '참을': 776,\n",
       " '수영하는': 777,\n",
       " '되었어': 778,\n",
       " '만났으면': 779,\n",
       " '걸려': 780,\n",
       " '파란': 781,\n",
       " '코트를': 782,\n",
       " '행동을': 783,\n",
       " '가수가': 784,\n",
       " '인간은': 785,\n",
       " '고기를': 786,\n",
       " '아파트에': 787,\n",
       " '말라고': 788,\n",
       " '좋았을': 789,\n",
       " '대부분의': 790,\n",
       " '새': 791,\n",
       " '제대로': 792,\n",
       " '타본': 793,\n",
       " '신앙심은': 794,\n",
       " '선호해': 795,\n",
       " '3': 796,\n",
       " '한국에서': 797,\n",
       " '없다고': 798,\n",
       " '아니다': 799,\n",
       " '쓸': 800,\n",
       " '해서': 801,\n",
       " '선물을': 802,\n",
       " '대화할': 803,\n",
       " '번도': 804,\n",
       " '피아노를': 805,\n",
       " '가까운': 806,\n",
       " '기타를': 807,\n",
       " '심각한': 808,\n",
       " '있었으면': 809,\n",
       " '좋았을텐데': 810,\n",
       " '요즘': 811,\n",
       " '할지': 812,\n",
       " '가진': 813,\n",
       " '있어야': 814,\n",
       " '외국에': 815,\n",
       " '오늘밤': 816,\n",
       " '한다고': 817,\n",
       " '일': 818,\n",
       " '대략': 819,\n",
       " '부탁했어': 820,\n",
       " '방법이': 821,\n",
       " '가치가': 822,\n",
       " '끔찍한': 823,\n",
       " '공을': 824,\n",
       " '때는': 825,\n",
       " '여름방학을': 826,\n",
       " '우리랑': 827,\n",
       " '유일한': 828,\n",
       " '알려지지': 829,\n",
       " '생각하고': 830,\n",
       " '최종': 831,\n",
       " '목표가': 832,\n",
       " '보스턴을': 833,\n",
       " '상상도': 834,\n",
       " '알려줄': 835,\n",
       " '마지막': 836,\n",
       " '때문에': 837,\n",
       " '테니스': 838,\n",
       " '생각은': 839,\n",
       " '달보다': 840,\n",
       " '인간의': 841,\n",
       " '마디': 842,\n",
       " '안했어': 843,\n",
       " '오래': 844,\n",
       " '맞는': 845,\n",
       " '어두워지기': 846,\n",
       " '아무것도': 847,\n",
       " '겪고': 848,\n",
       " '않네': 849,\n",
       " '이빨에': 850,\n",
       " '끼어있어': 851,\n",
       " '특별히': 852,\n",
       " '경향이': 853,\n",
       " '공부를': 854,\n",
       " '수영하러': 855,\n",
       " '인간': 856,\n",
       " '이름을': 857,\n",
       " '위해서': 858,\n",
       " '크리스마스를': 859,\n",
       " '선생님이': 860,\n",
       " '음식을': 861,\n",
       " '하기': 862,\n",
       " '친구가': 863,\n",
       " '선물로': 864,\n",
       " '말하더라': 865,\n",
       " '어렸을': 866,\n",
       " '예전만큼': 867,\n",
       " '개의': 868,\n",
       " '고등학교': 869,\n",
       " '있을지': 870,\n",
       " '중에서': 871,\n",
       " '울음을': 872,\n",
       " '관점으로': 873,\n",
       " '딱': 874,\n",
       " '잠깐': 875,\n",
       " '알겠어': 876,\n",
       " '어서': 877,\n",
       " '보여줘': 878,\n",
       " '설거지': 879,\n",
       " '계속해': 880,\n",
       " '서둘러': 881,\n",
       " '되네': 882,\n",
       " '거짓말했어': 883,\n",
       " '그만둬': 884,\n",
       " '하러': 885,\n",
       " '새가': 886,\n",
       " '있네': 887,\n",
       " '따라와': 888,\n",
       " '잊어': 889,\n",
       " '기절했어': 890,\n",
       " '내렸어': 891,\n",
       " '가져와': 892,\n",
       " '와인을': 893,\n",
       " '골라': 894,\n",
       " '절대로': 895,\n",
       " '사람에': 896,\n",
       " '우릴': 897,\n",
       " '우리를': 898,\n",
       " '신은': 899,\n",
       " '존재해': 900,\n",
       " '걔': 901,\n",
       " '킬킬': 902,\n",
       " '부러워': 903,\n",
       " '원해': 904,\n",
       " '엄마가': 905,\n",
       " '엄마는': 906,\n",
       " '불러': 907,\n",
       " '사기': 908,\n",
       " '춤을': 909,\n",
       " '췄어': 910,\n",
       " '실패했어': 911,\n",
       " '싸웠어': 912,\n",
       " '쳐다봤어': 913,\n",
       " '고개를': 914,\n",
       " '쉬었어': 915,\n",
       " '불렀어': 916,\n",
       " '안으로': 917,\n",
       " '참': 918,\n",
       " '안해': 919,\n",
       " '개는': 920,\n",
       " '좋네': 921,\n",
       " '죽지': 922,\n",
       " '놓아': 923,\n",
       " '기억해': 924,\n",
       " '움직이지': 925,\n",
       " '기침했어': 926,\n",
       " '걸어': 927,\n",
       " '당신은': 928,\n",
       " '가도': 929,\n",
       " '운전해': 930,\n",
       " '조사해봐': 931,\n",
       " '사과했어': 932,\n",
       " '땅': 933,\n",
       " '운전하고': 934,\n",
       " '집중하고': 935,\n",
       " '바빴다': 936,\n",
       " '거절했어': 937,\n",
       " '빨라': 938,\n",
       " '무엇입니까': 939,\n",
       " '죽어': 940,\n",
       " '약혼했어': 941,\n",
       " '사용해': 942,\n",
       " '죽일': 943,\n",
       " '끝났어': 944,\n",
       " '피야': 945,\n",
       " '싸워': 946,\n",
       " '모습을': 947,\n",
       " '총': 948,\n",
       " '주저했어': 949,\n",
       " '가까이서': 950,\n",
       " '빨개': 951,\n",
       " '가을이': 952,\n",
       " '드릴까요': 953,\n",
       " '일어섰어': 954,\n",
       " '받아': 955,\n",
       " '재밌어': 956,\n",
       " '책': 957,\n",
       " '야옹하고': 958,\n",
       " '얼음이': 959,\n",
       " '녹았어': 960,\n",
       " '저를': 961,\n",
       " '랩': 962,\n",
       " '내게': 963,\n",
       " '운전': 964,\n",
       " '조심하세요': 965,\n",
       " '우울하다': 966,\n",
       " '느꼈어': 967,\n",
       " '받았다': 968,\n",
       " '마음이': 969,\n",
       " '연필이야': 970,\n",
       " '찼습니다': 971,\n",
       " '원한다': 972,\n",
       " '우리에겐': 973,\n",
       " '즉시': 974,\n",
       " '물고기': 975,\n",
       " '노래하기를': 976,\n",
       " '머리카락은': 977,\n",
       " '부럽지': 978,\n",
       " '언어가': 979,\n",
       " '미안하다고': 980,\n",
       " '문': 981,\n",
       " '해요': 982,\n",
       " '어두워지고': 983,\n",
       " '관리자야': 984,\n",
       " '포기할': 985,\n",
       " '해낼': 986,\n",
       " '와서': 987,\n",
       " '밖에서': 988,\n",
       " '엄마': 989,\n",
       " '잘자': 990,\n",
       " '사랑을': 991,\n",
       " '시에': 992,\n",
       " '일어났어': 993,\n",
       " '웃는': 994,\n",
       " '훔쳤어': 995,\n",
       " '영화를': 996,\n",
       " '낯을': 997,\n",
       " '먹이를': 998,\n",
       " '사전이다': 999,\n",
       " '시계를': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lang_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3318, 103)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3318, 91)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(input_tensor_train)\n",
    "batch_size = 50\n",
    "steps_per_epoch = len(input_tensor_train)//batch_size\n",
    "embedding_dim = 50\n",
    "units = 100\n",
    "\n",
    "vocab_inp_size = len(input_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(target_lang_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2355"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5104"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(Model):\n",
    "#     def __init__(self, vocab_size, embed_dim, enc_units, batch_size):\n",
    "#         super(Encoder,self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.enc_units = enc_units\n",
    "#         self.batch_size = batch_size\n",
    "        \n",
    "#         self.embedding_layer = Embedding(self.vocab_size,self.embed_dim)\n",
    "#         self.gru_layer = GRU(self.enc_units, return_sequences=True,return_state=True)\n",
    "        \n",
    "#     def call(self,x,hidden):\n",
    "#         x = self.embedding_layer(x)\n",
    "#         output,state = self.gru_layer(x,initial_state=hidden)\n",
    "#         return output, state\n",
    "    \n",
    "#     def initialize_hidden_state(self):\n",
    "#         return tf.zeros((self.batch_size,self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=vocab_inp_size, embed_dim=embedding_dim, enc_units=units, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_hidden = encoder.initialize_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.W1 = Dense(units)\n",
    "#         self.W2 = Dense(units)\n",
    "#         self.V = Dense(1)\n",
    "        \n",
    "#     def call(self, query, values):\n",
    "#         hidden_with_time = tf.expand_dims(query,1)\n",
    "#         score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time)))\n",
    "        \n",
    "#         attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "#         context_vector = attention_weights * values\n",
    "#         context_vector = tf.reduce_sum(context_vector,axis=1)\n",
    "        \n",
    "#         return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = Attention(10)\n",
    "# attention_vector, attention_weights = attention_layer(sample_hidden,sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(Model):\n",
    "#     def __init__(self, vocab_size, embed_dim, dec_units, batch_size):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.dec_units = dec_units\n",
    "#         self.batch_size = batch_size\n",
    "        \n",
    "#         self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "#         self.gru_layer = GRU(self.dec_units, return_sequences=True,return_state=True)\n",
    "#         self.dense = Dense(vocab_size)\n",
    "        \n",
    "#         self.attention = Attention(self.dec_units)\n",
    "        \n",
    "#     def call(self, x, hidden, enc_output):\n",
    "#         attention_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "#         x = self.embedding_layer(x)\n",
    "        \n",
    "#         x = tf.concat([tf.expand_dims(attention_vector, 1),x],axis=-1)\n",
    "        \n",
    "#         output, state = self.gru_layer(x)\n",
    "#         output = tf.reshape(output,(-1,output.shape[2]))\n",
    "#         x = self.dense(output)\n",
    "        \n",
    "#         return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),sample_hidden, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "# def loss_function(real,pred):\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "#     loss_ = loss_object(real,pred)\n",
    "    \n",
    "#     mask = tf.cast(mask,dtype=loss_.dtype)\n",
    "#     loss_ *= mask\n",
    "    \n",
    "#     return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8292131, shape=(50, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @tf.function\n",
    "# def train_step(inp, targ, enc_hidden):\n",
    "    \n",
    "#     loss = 0\n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         enc_output, enc_hidden_state = encoder(inp, enc_hidden)\n",
    "        \n",
    "#         dec_hidden_state = enc_hidden_state\n",
    "        \n",
    "#         dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "        \n",
    "#         for t in range(1, targ.shape[1]):\n",
    "#             pred, dec_hidden, _ = decoder(dec_input, dec_hidden_state, enc_output)\n",
    "#             loss += loss_function(targ[:,t],pred)\n",
    "#             dec_input = tf.expand_dims(targ[:,t],1)\n",
    "            \n",
    "#     batch_loss = (loss/int(targ.shape[1]))\n",
    "#     variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "#     gradients = tape.gradient(loss, variables)\n",
    "#     optimizer.apply_gradients(zip(gradients,variables))\n",
    "    \n",
    "#     return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6515\n",
      "Epoch 1 Batch 50 Loss 0.3950\n",
      "Epoch 1 Loss 0.4146\n",
      "Epoch 2 Batch 0 Loss 0.4033\n",
      "Epoch 2 Batch 50 Loss 0.3665\n",
      "Epoch 2 Loss 0.3805\n",
      "Epoch 3 Batch 0 Loss 0.3656\n",
      "Epoch 3 Batch 50 Loss 0.4043\n",
      "Epoch 3 Loss 0.3782\n",
      "Epoch 4 Batch 0 Loss 0.3588\n",
      "Epoch 4 Batch 50 Loss 0.3887\n",
      "Epoch 4 Loss 0.3801\n",
      "Epoch 5 Batch 0 Loss 0.3616\n",
      "Epoch 5 Batch 50 Loss 0.3750\n",
      "Epoch 5 Loss 0.3891\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp,targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp,targ, enc_hidden,encoder,decoder,target_lang_tokenizer,batch_size)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % batch_size == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(sentence):\n",
    "#     attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "#     input_sentence = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "#     input_sentence = pad_sequences([input_sentence],maxlen=max_length_inp,padding='post')\n",
    "    \n",
    "    \n",
    "#     input_sentence_tensor = tf.convert_to_tensor(input_sentence)\n",
    "#     result = ''\n",
    "    \n",
    "#     hidden = [tf.zeros((1,units))]\n",
    "#     enc_output, enc_hidden = encoder(input_sentence_tensor, hidden)\n",
    "    \n",
    "#     dec_hidden = enc_hidden\n",
    "#     dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']],0)\n",
    "    \n",
    "#     for t in range(max_length_targ):\n",
    "#         pred, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "#         attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "#         attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "#         pred_id = tf.argmax(pred[0]).numpy()\n",
    "        \n",
    "#         result += target_lang_tokenizer.index_word[pred_id] + ' '\n",
    "        \n",
    "#         if target_lang_tokenizer.index_word[pred_id] == '<end>':\n",
    "#             return result, sentence, attention_plot\n",
    "        \n",
    "#         dec_input = tf.expand_dims([pred_id],0)\n",
    "#     return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_attention(attention, sentence, pred):\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(1, 1, 1)\n",
    "#     ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "#     fontdict={'fontsize':14}\n",
    "    \n",
    "#     ax.set_xticklabels([''] + sentence, fontdict=fontdict)\n",
    "#     ax.set_yticklabels([''] + pred, fontdict=fontdict)\n",
    "    \n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  117750    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  45600     \n",
      "=================================================================\n",
      "Total params: 163,350\n",
      "Trainable params: 163,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1 1175    2    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n",
      "tf.Tensor(\n",
      "[[   1 1175    2    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]], shape=(1, 103), dtype=int32)\n",
      "Input: <start> floor <end>\n",
      "Predicted: 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 충분했더라면 영어는 바닥에 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result,sentence,attention_plot = evaluate('floor',max_length_targ,max_length_inp,encoder,decoder,input_lang_tokenizer,target_lang_tokenizer,units)\n",
    "\n",
    "print(f\"Input: {sentence}\")\n",
    "print(f\"Predicted: {result}\")\n",
    "\n",
    "attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attention_plot,sentence.split(' '),result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"tom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.read_csv('data/lyrics.txt', sep='\\t')\n",
    "\n",
    "# idx_to_drop = [19565, 28696, 31890]\n",
    "\n",
    "# all_df = all_df.drop(idx_to_drop)\n",
    "# all_df['lang'] = all_df['kor'].apply(detect)\n",
    "\n",
    "# all_df = all_df[all_df['lang']=='ko'].drop(columns=['lang','Unnamed: 0'])\n",
    "\n",
    "# df_100 = pd.read_csv('data/lyrics_save100.txt',sep='\\t')\n",
    "\n",
    "# drop = [2096, 9030]\n",
    "# df_100 = df_100.drop(drop)\n",
    "\n",
    "# df_100['lang'] = df_100['kor'].apply(detect)\n",
    "\n",
    "# df_100 = df_100[df_100['lang']=='ko'].drop(columns=['lang','Unnamed: 0'])\n",
    "\n",
    "# final_final = pd.concat([df_100,all_df,df_test])\n",
    "\n",
    "# final_final = final_final.reset_index().drop(columns='index')\n",
    "\n",
    "# final_final.to_csv('data/final_df.txt',sep='\\t')\n",
    "# final_final.to_csv('data/final_df.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
